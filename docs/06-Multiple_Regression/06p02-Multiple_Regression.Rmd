---
title: "CEMA 0928: Statistics in the Real World"
subtitle: "Multiple Regression (PART 2)"
author: "Anthony Scotina"
date: 
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: my-theme.css
    nature:
      countIncrementalSlides: false
      highlightLines: true
---

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_accent(base_color = "#4682B4") #3E8A83?
```

```{r, include = FALSE}
library(tidyverse)
library(moderndive)
```

# Needed Packages

```{r}
library(tidyverse)
library(moderndive)
```

---

# Taking care of the outlier...

It is reasonable to suspect that the **outlier** with 33 bedrooms is *not representative* of the population in the same way that the rest of the sample is. 

- Let's remove the outlier to see if `bedrooms` and `price` are more *linearly related*:

```{r}
outlier_index = which(house_prices$bedrooms == 33) #15871
house_prices = 
  house_prices[-outlier_index, ]
```

- This removes the 15,871st row, which contains the outlier, from the data. 

---

class: center, middle

# Parallel Slopes Model

---

# Parallel Slopes Model

When creating **multiple regression models** with *one numerical* and *one categorical* explanatory variable, we are *not* limited to models with an **interaction effect**. 

- Another type of model we can use is known as the **parallel slopes model**. 

--

While **interaction models** can have regression lines with *different slopes and intercepts*, parallel slopes models *force* all lines to have the **same slope**. 

---

# Visualizing Parallel Slopes

The `gg_parallel_slopes()` function in the `moderndive` package provides one way to plot a **parallel slopes model**. 

```{r, message = FALSE, warning = FALSE, out.width = "35%"}
gg_parallel_slopes(y = "price", num_x = "bedrooms", 
                   cat_x = "waterfront", data = house_prices)
```

- **Note**: The *parallel regression lines* are not necessarily the **lines of best fit**, which is why `xyplot()` can't really help us here!

---

# Parallel Slopes Model

.pull-left[
```{r, message = FALSE, warning = FALSE, echo = FALSE}
gg_parallel_slopes(y = "price", num_x = "bedrooms", 
                   cat_x = "waterfront", data = house_prices)
```
]

.pull-right[
Here, waterfront and non-waterfront homes have the *same slope*. 

- They still have *different intercepts*, which is why the lines are at different heights. 

- Irrespective of the number of bedrooms, *waterfront homes* tended to be more expensive than *non-waterfront homes*, on average. 
]

---

# Fitting the Parallel Slopes Model

```{r}
lm_price_parallel = lm(price ~ bedrooms + waterfront, 
                       data = house_prices)
get_regression_table(lm_price_parallel)
```

The equation for the linear model is: $$\widehat{price}=99306+128265(bedrooms)+1139217(waterfrontTRUE)$$

---

# Interpreting the Coefficients

$$\widehat{price}=99306+128265(bedrooms)+1139217(waterfrontTRUE)$$

The coefficients $b_{0}=99306$ and $b_{2}=1139217$ acts as they did in the *interaction model*: 

- $b_{0}=99306$ is the `intercept` for *non-waterfront homes*.
    - The **average** price is <span>&#36;</span>99,306 for *non-waterfront homes* with 0 *bedrooms*.

--

- $b_{2}=1139217$ is the **offset** in the intercept for *waterfront homes*. 
    - The **average** price is <span>&#36;</span>99,306+<span>&#36;</span>1,139,217 = <span>&#36;</span>1,238,523 for *waterfront homes* with 0 *bedrooms*. 

---

# Interpreting the Coefficients

$$\widehat{price}=99306+128265(bedrooms)+1139217(waterfrontTRUE)$$

Unlike in the *interaction model*, there is only one slope term in the *parallel slopes model*:

- $b_{1}=128265$ is the slope for *waterfront* **and** *non-waterfront homes*. 
    - **Holding** `waterfront` **fixed** (i.e., for *one level of* `waterfront`): For every additional `bedroom`, there is an associated increase of, **on average**, <span>&#36;</span>128,265 on the price of the home. 

---

# Parallel Slopes Model

$$\widehat{price}=99306+128265(bedrooms)+1139217(waterfrontTRUE)$$

For *waterfront homes*: $$\widehat{price}=1238523+\underline{128265}(bedrooms)$$

For *non-waterfront homes*: $$\widehat{price}=99306+\underline{128265}(bedrooms)$$

---

# Comparing the Models

```{r, echo = FALSE, out.width = "50%"}
ggplot(house_prices, aes(x = bedrooms, y = price, color = waterfront)) + 
  geom_point() + 
  labs(x = "Bedrooms per home", y = "Price (in $)", title = "Interaction Model") + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw()
ggplot(house_prices, aes(x = bedrooms, y = price, color = waterfront)) + 
  geom_point() + 
  geom_parallel_slopes(se = FALSE) +
  labs(x = "Bedrooms per home", y = "Price (in $)", title = "Parallel Slopes Model") + 
  theme_bw() 
```

So... why would we *ever* use a **parallel slopes model**?
- The lines in the left-hand plot don't look parallel, so why force them to be?
- We'll get back to this, but as a short answer: Sometimes **simple** is better!

---

# Practice

The `Credit` dataset (on Moodle, and in the `ISLR` package) contains financial data on a sample of $n=400$ individuals. 

```{r, eval = FALSE}
View(Credit)
```

Using these data, model `Balance` (credit card balance) as a function of `Income` (income in 10,000 dollars) and `Student` (whether or not the individual is a student):
- *y* = `Balance`
- $x_{1}$ = `Income`
- $x_{2}$ = `Student`

Using appropriate visualizations, decide whether an interaction model is appropriate. 

---

class: center, middle

# Two (or more) Numerical Explanatory Variables

---

# Two Numerical Explanatory Variables

Instead of examining a model with *one numerical* and *one categorical* explanatory, let's look at a model with **two numerical explanatory variables**. 

Using the `house_prices` data:

- *y* = `price`

- $x_{1}$ = `bedrooms`

- $x_{2}$ = `sqft_living` (square footage of the home, from `?house_prices`)

In other words, we will fit the model: $$\widehat{price} = b_{0}+b_{1}(bedrooms)+b_{2}(sqft.living)$$

---

# Correlation Matrix

Because we have several **numerical** variables, we can calculate several pairwise **correlation coefficients**. 
- (Recall that one cannot calculate a correlation coefficient with categorical variables.)

While we could do this with `cor()` several times depending on the number of comparisons, it is much more efficient to construct a **correlation matrix**:

```{r}
house_prices %>%
  select(price, bedrooms, sqft_living) %>%
  cor()
```

---

# Correlation Matrix

```{r}
house_prices %>%
  select(price, bedrooms, sqft_living) %>%
  cor()
```

- $r(price,bedrooms)=0.32$. 

--

- $r(price, sqft.living)=0.70$ means that there is a *relatively strong*, positive, linear correlation between price per home and square footage of the home. 

--

- $r(bedrooms, sqft.living)=0.59$ means that there is a *moderate*, positive, linear correlation between number of bedrooms per home and square footage of the home. 
    - When two **explanatory variables** are correlated, we say that there is a high degree of **multicollinearity**. 

---

# Data Visualizations

.pull-left[
```{r}
ggplot(data = house_prices, aes(x = sqft_living, y = price)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  labs(x = "Square Footage of Home", y = "Price (in $)")
```
]

.pull-right[
```{r}
ggplot(data = house_prices, aes(x = bedrooms, y = sqft_living)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  labs(x = "Number of Bedrooms", y = "Square Footage of Home")
```
]

---

# Fitting the Model

To fit a model of this form in R, we use `lm()` exactly as we did in previous examples:

```{r}
lm_multiple = lm(price ~ bedrooms + sqft_living, data = house_prices)
get_regression_table(lm_multiple)
```

The equation of the *regression plane* is: $$\widehat{price}=90034-62062(bedrooms)+317(sqft.living)$$

---

# Interpreting the Regression Coefficients

$$\widehat{price}=90034-62062(bedrooms)+317(sqft.living)$$

- $b_{0}=90034$: The average price is <span>&#36;</span>90,034 for homes with *0 bedrooms* **AND** *0 square footage of space*. 
    - This doesn't make sense in context; using `sqft_living=0` is **extrapolation**!
    
--

- $b_{1}=-62062$: *Taking all other variables in our model into account* (i.e., holding them fixed), for every additional bedroom, there is an associated **decrease** of <span>&#36;</span>62,062 in price per home, on average. 

--

- $b_{2}=317$: *Taking all other variables in our model into account*, for every additional square foot of living space, there is an associated **increase** of <span>&#36;</span>317 in price per home, on average. 

---

# Interpreting the Regression Coefficients

To better understand what these interpretations mean, let's consider a **simple regression model** and a **multiple regression model** side-by-side:

```{r}
lm_simple = lm(price ~ bedrooms, data = house_prices)
get_regression_table(lm_simple)

lm_multiple = lm(price ~ bedrooms + sqft_living, data = house_prices)
get_regression_table(lm_multiple)
```

---

# Interpreting the Regression Coefficients

- **Simple**: $\widehat{price}=110316+127548(bedrooms)$
- **Multiple**: $\widehat{price}=90034-62062(bedrooms)+317(sqft.living)$

Introducing `sqft_living` into the model yielded a $b_{1}$ coefficient with **opposite sign**!
- Changed from $127548$ to $-62062$

--

In the multiple regression model, the estimated coefficient $b_{1}=-62062$ does not mean that homes with *more bedrooms* are generally *worth less*. 
- It must be interpreted while taking into account the *other explanatory variable* (`sqft_living`). 

--

Let's consider a home with a *fixed amount of living area*:
- Those which devote more area to bedrooms must either (a) have smaller bedrooms; or (b) have less living area
- This could reduce the home's value!
